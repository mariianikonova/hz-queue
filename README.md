hz-queue
========

Tests of hazelcast queue

hazelcast 3.2.3

## Plan

ХЗ - хороший продукт. Общие декларации.. На больших и критичных задачах для бизнеса нам нужно точное определение 
применимости и без испытаний не обойтись.

Параметры JVM для всех тестов:
- ограничиваем память
- сбрасываем дамп при OOM
- убиваем приложение при OOM

-Xms64m -Xmx64m -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/tmp -XX:OnOutOfMemoryError="kill -9 %p"



## Test1

Один узел, все в паняти

460k 0.026 - OOM 

## Test2

Один узел, MockQueueStore

980k 0.030 - OOM

        public long offer(Data data) {
            QueueItem item = new QueueItem(this, nextId(), null);
            if (store.isEnabled()) {
                try {
                    store.store(item.getItemId(), data);
                } catch (Exception e) {
                    throw new HazelcastException(e);
                }
            }
            if (!store.isEnabled() || store.getMemoryLimit() > getItemQueue().size()) {
                item.setData(data);
            }
            getItemQueue().offer(item);
            cancelEvictionIfExists();
            return item.getItemId();
        }
        
# Test3

Один узел, MockQueueStore, транзакции

250k 0.061 - OOM

# Test4 

Два узла (по умолчанию) - все в памяти. 100 тыс записей.
Пишем и читаем с владельца очереди.

INFO: add 100000	   0.208
INFO: poll 100000	   0.197

По умолчанию включен синхронный бэкап на 1 узел. 
Видим потерю производительности на порядок. Владелец очереди тратит ресурсы на бэкап.

# Test 4_1

Тоже что Test4 но пишем и читаем с другого узла (не владельца очереди)

INFO: add 100000	   0.211
INFO: poll 100000	   0.201

Видим сравнимую скорость. Также как в тесте Test4 оба узла выполняют схожий набор операций и от перемены мест 
слагаемых результат практически не меняется. Фактическая работа идет с владельцем очереди.

# Test5

Тоже что Test4-1 + Роняем владельца очерди после ее заполнения
и проверяем очередь читая с другого узла.

INFO: add 100000	   0.267
INFO: poll 100000	   0.023

Один оставшийся узел начинает работать на порядок быстрее. Он один в кластере и не тратить ресурсы на коммуникации по
 созданию бэкапа.

# Test6

Тоже что и Test5, но без бэкапа

INFO: add 100000	   0.022

Большая скрость работы с владельцем очереди. Он не тратит ресурсы на бэкап. Но после его падения - вся очередь теряется.

# Test7

Тоже что и Test6, но с подключенным хранилищем в памяти.

INFO: add 100000	   0.023
INFO: poll 100000	   0.018

Видим что очередь восстановилась на оставшемся узле.
В процессе восстановления очерди сначала считываются все ключи из хранилища и определяется наибольшее значение из них
 для дальнейшей генерации новых. Сразу всеми элементами очереди, но без данных заполняется внутреннее хранилище списка 
 LinkedList. Для того чтобы порядок элементов в очереди после восстановления из хранилища не менялся - хранилище 
 должно выдавать их в правильном порядке в наборе (Set). Далее по необходимости подгружаются данные из хранилища. 
 Подгрузка идет пачками. По умолчанию по 250 штук.  





